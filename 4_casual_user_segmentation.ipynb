{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e307b4-b1b7-444a-9ed8-e6322d8681d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7560cf-525c-46f3-98e9-de0c9e0dff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('2023_dataset.csv')\n",
    "df.head()\n",
    "\n",
    "# Drop rows with missing values in key columns\n",
    "df = df.dropna(subset=['ride_id', 'started_at', 'ended_at', 'start_lat', 'start_lng', 'end_lat', 'end_lng'])\n",
    "\n",
    "# Function to correct year format\n",
    "def correct_year(date_str):\n",
    "    if isinstance(date_str, str):\n",
    "        return re.sub(r'^0{2,}23', '2023', date_str)\n",
    "    return date_str\n",
    "\n",
    "# Apply year correction\n",
    "df['started_at'] = df['started_at'].apply(correct_year)\n",
    "df['ended_at'] = df['ended_at'].apply(correct_year)\n",
    "\n",
    "# Convert to datetime format\n",
    "df['started_at'] = pd.to_datetime(df['started_at'])\n",
    "df['ended_at'] = pd.to_datetime(df['ended_at'])\n",
    "\n",
    "# Calculate ride duration in minutes\n",
    "df['ride_length_minutes'] = (df['ended_at'] - df['started_at']).dt.total_seconds() / 60\n",
    "\n",
    "# Filter out negative or extreme values (less than 1 min or more than 24 hrs)\n",
    "df = df[(df['ride_length_minutes'] > 1) & (df['ride_length_minutes'] < 1440)]\n",
    "\n",
    "# Add time-related fields\n",
    "df['weekday'] = df['started_at'].dt.day_name()\n",
    "df['month'] = df['started_at'].dt.month\n",
    "df['start_hour'] = df['started_at'].dt.hour\n",
    "\n",
    "# Calculate ride distance (in miles) using Haversine formula\n",
    "def calculate_distance(row):\n",
    "    start = (row['start_lat'], row['start_lng'])\n",
    "    end = (row['end_lat'], row['end_lng'])\n",
    "    return geodesic(start, end).miles\n",
    "\n",
    "df['distance_miles'] = df.apply(calculate_distance, axis=1)\n",
    "\n",
    "# Filter for casual users\n",
    "casual_df = df[df['member_casual'] == 'casual'].copy()\n",
    "casual_df['duration_min'] = casual_df['ride_length_minutes']\n",
    "casual_df['hour'] = casual_df['start_hour']\n",
    "casual_df['weekday_name'] = casual_df['weekday']\n",
    "\n",
    "# Construct user behavior features\n",
    "user_features = casual_df.groupby('ride_id').agg({\n",
    "    'duration_min': 'mean',\n",
    "    'distance_miles': 'mean',\n",
    "    'hour': lambda x: x.mode()[0] if not x.mode().empty else x.median(),\n",
    "    'weekday_name': lambda x: x.mode()[0] if not x.mode().empty else x.sample(1).values[0],\n",
    "    'start_lat': 'median',\n",
    "    'start_lng': 'median'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns\n",
    "user_features.columns = ['ride_id', 'avg_duration', 'avg_distance', 'common_hour', 'common_day', 'median_lat', 'median_lng']\n",
    "\n",
    "# Select features for clustering\n",
    "features = ['avg_duration', 'avg_distance', 'common_hour', 'median_lat', 'median_lng']\n",
    "X = user_features[features]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Elbow method to determine optimal K\n",
    "sse = []\n",
    "k_range = range(1, 10)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "# Plot SSE vs. K\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(k_range, sse, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('SSE')\n",
    "plt.title('Elbow Method for Determining Optimal K')\n",
    "plt.show()\n",
    "\n",
    "# Apply MiniBatch K-Means clustering\n",
    "k = 2  # Set to optimal K if determined\n",
    "mbk = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=1000)\n",
    "user_features['cluster'] = mbk.fit_predict(X_scaled)\n",
    "\n",
    "# Cluster feature summary\n",
    "cluster_summary = user_features.groupby('cluster')[features].mean().round(2)\n",
    "print(cluster_summary)\n",
    "\n",
    "# Optional: PCA for 2D visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "user_features['pca1'] = X_pca[:, 0]\n",
    "user_features['pca2'] = X_pca[:, 1]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for c in range(k):\n",
    "    subset = user_features[user_features['cluster'] == c]\n",
    "    plt.scatter(subset['pca1'], subset['pca2'], label=f'Cluster {c}', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.title(\"MiniBatch K-Means Cluster Visualization (PCA)\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
